#!/usr/bin/env python3
"""MIRAGE Web Interface - Interactive visualization for attribution-based citation."""

import asyncio
import gc
import logging
import re
import string
from concurrent.futures import ThreadPoolExecutor
from typing import Any

import inseq
import nltk
import numpy as np
import torch
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from inseq.commands.attribute_context.attribute_context import AttributeContextArgs, attribute_context_with_model
from nltk import sent_tokenize
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

app = FastAPI(title="MIRAGE Citation Visualization")
templates = Jinja2Templates(directory="templates")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Thread pool for blocking operations
executor = ThreadPoolExecutor(max_workers=2)

class Document(BaseModel):
    """Represents a document with a title and its text content.

    Attributes:
        title (str): The title of the document.
        text (str): The full text of the document.
    """
    title: str
    text: str

class ProcessRequest(BaseModel):
    """Schema for processing request parameters.

    Attributes:
        instruction (str): The system instruction guiding processing.
        question (str): The user question to address.
        documents (list[Document]): A list of documents to process.
        model (str): The model identifier to use (default: "Qwen/Qwen3-0.6B").
        cti_threshold (int): The CTI threshold (default: 1).
        cci_threshold (int): The CCI threshold (default: -5).
        temperature (float): Sampling temperature (default: 1.0).
        max_new_tokens (int): Maximum number of new tokens (default: 500).
        output (Optional[str]): Optional initial output value.
    """
    instruction: str
    question: str
    documents: list[Document]
    model: str = "Qwen/Qwen3-0.6B"
    cti_threshold: int = 1
    cci_threshold: int = -5
    temperature: float = 1.0
    max_new_tokens: int = 500
    output: str | None = None  # Optional output field

class ProcessResponse(BaseModel):
    """Schema for the response returned after processing.

    Attributes:
        output (str): The main output generated by the process.
        citations (list[dict[str, Any]]): list of citation metadata.
        document_highlights (list[dict[str, Any]]): Highlights extracted from documents.
        attribution_scores (Optional[dict[str, Any]]): Optional attribution scores.
        token_highlights (Optional[list[dict[str, Any]]]): Optional token-level highlights.
    """
    output: str
    citations: list[dict[str, Any]]
    document_highlights: list[dict[str, Any]]
    attribution_scores: dict[str, Any] | None = None
    token_highlights: list[dict[str, Any]] | None = None

# Import MIRAGE utility functions
# from mirage import process_with_mirage  # Uncomment and adjust import as needed

def normalize_answer(s: str) -> str:
    """Normalize an answer string by removing articles and extra whitespace.

    Args:
        s (str): The string to normalize.

    Returns:1
        str: The normalized string.
    """
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def get_max_memory():
    """Get the maximum memory available for the current GPU for loading models."""
    if torch.cuda.is_available():
        free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)
        max_memory = f'{free_in_GB-2}GB'  # Réduit de 6GB à 2GB pour plus de marge
        n_gpus = torch.cuda.device_count()
        max_memory = {i: max_memory for i in range(n_gpus)}
        return max_memory
    else:
        return None

def clear_gpu_memory():
    """Force complete GPU memory cleanup."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # Log memory status
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        logger.info(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")

def make_doc_prompt(doc, doc_id, doc_prompt):
    """Format a document for the prompt."""
    return doc_prompt.replace("{T}", doc.get("title", "")).replace("{P}", doc.get("text", "")).replace("{ID}", str(doc_id+1))

def remove_citations(sent):
    """Remove existing citations from a sentence."""
    return re.sub(r"\[\d+", "", re.sub(r" \[\d+", "", sent)).replace(" |", "").replace("]", "")

def clean_thinking_output(text):
    """Remove thinking tags from output."""
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = text.replace('<think>', '').replace('</think>', '')
    return text.strip()

def mirage_cite(res_mirage, cti_threshold, start_pos_sent, end_pos_sent, topk_CCI, doc_seps):
    """MIRAGE citation algorithm."""
    res = []
    sum_value = np.zeros(len(res_mirage['input_context_tokens']))
    
    for i in res_mirage['cci_scores']:
        # CTI Filtering
        if not (i["cti_idx"] >= start_pos_sent and i["cti_idx"] < end_pos_sent): 
            continue
        if i['cti_score'] >= cti_threshold:
            # CCI Focus
            CCI_value = np.array(i['input_context_scores'])
            if topk_CCI == 0:
                cci_threshold = np.mean(CCI_value)
            elif topk_CCI < 0:
                cci_threshold = (1+topk_CCI/100) * np.max(CCI_value) - topk_CCI/100 * np.min(CCI_value)
            else:
                cci_threshold = np.sort(CCI_value)[-topk_CCI]
            zero_idx = CCI_value < cci_threshold
            CCI_value[zero_idx] = 0
            sum_value += CCI_value

        if i['cti_score'] < cti_threshold: 
            break

    sum_tmp = 0
    for i, v in enumerate(sum_value):
        sum_tmp += v
        if doc_seps[i] or (i == len(sum_value)-1):
            res.append(sum_tmp)
            sum_tmp = 0
    return res

async def process_with_mirage(request: ProcessRequest) -> dict[str, Any]:
    """Process documents with MIRAGE in a non-blocking way."""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, _process_with_mirage_sync_debug, request)
    
def _process_with_mirage_sync_debug(request: ProcessRequest) -> dict[str, Any]:
    """Version avec debug amélioré."""
    output_text = None
    
    try:
        clear_gpu_memory()
        config = create_config_dict(request)
        
        data_item = {
            "question": request.question,
            "docs": [{"title": doc.title, "text": doc.text} for doc in request.documents]
        }
        
        # Générer ou utiliser la sortie fournie
        if request.output:
            logger.info("Using provided output, skipping generation step")
            output_text = request.output
        else:
            logger.info("Loading model for generation...")
            model, tokenizer = load_model_for_generation(config['model'])
            
            try:
                output_text = generate_response(data_item, model, tokenizer, config)
            finally:
                del model
                del tokenizer
                clear_gpu_memory()
                import time
                time.sleep(2)
        
        # Analyser avec MIRAGE
        logger.info("Analyzing with MIRAGE...")
        attribution_data = analyze_with_mirage(data_item, output_text, config)
        
        # Ajouter les citations avec debug
        logger.info("Adding citations with detailed logging...")
        cited_output, doc_scores_per_sentence, all_doc_scores = add_citations_to_output_with_debug(
            output_text, attribution_data, data_item, config
        )
        
        # Résumé des scores
        logger.info("\n=== SCORE SUMMARY ===")
        for sent_idx, scores in all_doc_scores.items():
            logger.info(f"Sentence {sent_idx}: {[f'Doc{i}:{s:.2f}' for i, s in enumerate(scores)]}")
        
        # Si pas de données d'attribution
        if not attribution_data.get("cti_scores"):
            logger.warning("No attribution data available, providing basic response")
            return {
                "output": output_text,
                "citations": [],
                "document_highlights": [
                    {
                        "doc_idx": idx,
                        "title": doc.title,
                        "highlights": [],
                        "full_text": doc.text
                    } for idx, doc in enumerate(request.documents)
                ],
                "attribution_scores": None,
                "token_highlights": None
            }
        
        # Extraire les highlights avec la version unifiée
        results = extract_citations_and_highlights(
            cited_output, attribution_data, request.documents, 
            doc_scores_per_sentence, all_doc_scores, config
        )
        
        # Token highlights
        tokenizer = AutoTokenizer.from_pretrained(config['model'], use_fast=False, trust_remote_code=True)
        
        try:
            token_highlights = compute_token_highlights(
                output_text, attribution_data, len(request.documents), tokenizer, config
            )
        finally:
            del tokenizer
            clear_gpu_memory()
        
        return {
            "output": cited_output,
            "citations": results["citations"],
            "document_highlights": results["document_highlights"],
            "attribution_scores": attribution_data,
            "token_highlights": token_highlights
        }
        
    except Exception as e:
        logger.error(f"Error in MIRAGE processing: {str(e)}")
        import traceback
        traceback.print_exc()
        
        clear_gpu_memory()
        
        return {
            "output": f"Error: {str(e)}",
            "citations": [],
            "document_highlights": [
                {
                    "doc_idx": idx,
                    "title": doc.title,
                    "highlights": [],
                    "full_text": doc.text
                } for idx, doc in enumerate(request.documents)
            ],
            "attribution_scores": None,
            "token_highlights": None
        }

def create_config_dict(request: ProcessRequest) -> dict[str, Any]:
    """Create configuration dictionary based on model."""
    base_config = {
        "model": request.model,
        "temperature": request.temperature,
        "top_p": 0.95,
        "max_new_tokens": request.max_new_tokens,
        "max_length": 32768,
        "instruction": request.instruction,
        "doc_prompt": "Document [{ID}](Title: {T}): {P}\n",
        "CTI": request.cti_threshold,
        "CCI": request.cci_threshold,
        "at_most_citations": 3,
        "seed": 42,
        "batch_size": 1,  # Process one at a time to save memory
        "gradient_checkpointing": True,  # Enable gradient checkpointing to save memory
    }
    
    # Model-specific configurations
    if "qwen" in request.model.lower():
        base_config.update({
            "use_chat_template": True,
            "enable_thinking": False,
            "demo_prompt": "<|im_start|>user\n{INST}\n\nQuestion: {Q}\n\n{D}\n<|im_end|>\n<|im_start|>assistant\n{A}<|im_end|>",
            "decoder_input_output_separator": "\n",
            "special_tokens_to_keep": ["<|im_end|>", "</think>"],
            "tokenizer_newline_token": "<0x0A>",
            "stop_tokens": ["<|im_end|>", "<|endoftext|>"],
        })
    elif "llama" in request.model.lower():
        base_config.update({
            "use_chat_template": False,
            "demo_prompt": "{INST}\n\nQuestion: {Q}\n\n{D}\nAnswer:{A}",
            "decoder_input_output_separator": " ",
            "tokenizer_newline_token": "Ċ",
        })
    else:
        base_config.update({
            "use_chat_template": False,
            "demo_prompt": "<|system|>\n{INST}</s>\n\n<|user|>\n{Q}\n\n{D}</s>\n\n<|assistant|>\n{A}</s>",
            "decoder_input_output_separator": "\n ",
            "special_tokens_to_keep": ["</s>"],
            "tokenizer_newline_token": "<0x0A>",
        })
    
    return base_config

def load_model_for_generation(model_name: str):
    """Load model and tokenizer for generation."""
    logger.info(f"Loading {model_name}...")
    
    # Force CPU offloading for large models
    device_map = "auto"
    max_memory = get_max_memory()
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map=device_map,
        max_memory=max_memory,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        offload_folder="./offload",  # Use disk offloading if needed
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
    
    # Fix OPT bos token problem in HF
    if "opt" in model_name:
        tokenizer.bos_token = "<s>"
    tokenizer.padding_side = "left"
    
    return model, tokenizer

def generate_response(data_item: dict, model, tokenizer, config: dict) -> str:
    """Generate response using the model."""
    doc_list = data_item['docs']
    input_context_text = "".join([make_doc_prompt(doc, doc_id, config['doc_prompt']) 
                                 for doc_id, doc in enumerate(doc_list)])
    input_current_text = data_item['question']
    
    if config.get('use_chat_template', False):
        # Chat template format
        chat_prompt = f"Question: {input_current_text}\n\nDocuments:\n{input_context_text}"
        messages = [{"role": "user", "content": chat_prompt}]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Prepare stop tokens
        stop_tokens = config.get('stop_tokens', [])
        stop_token_ids = []
        
        if stop_tokens:
            for token in stop_tokens:
                token_id = tokenizer.convert_tokens_to_ids(token)
                if token_id and token_id != tokenizer.unk_token_id:
                    stop_token_ids.append(token_id)
        
        if not stop_token_ids and hasattr(model.config, 'eos_token_id'):
            stop_token_ids = [model.config.eos_token_id]
        
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        # Generate with memory-efficient settings
        with torch.cuda.amp.autocast():
            outputs = model.generate(
                **inputs,
                do_sample=True,
                temperature=config['temperature'],
                top_p=config['top_p'],
                max_new_tokens=config['max_new_tokens'],
                eos_token_id=stop_token_ids if stop_token_ids else None,
                use_cache=True,
            )
        
        # Handle Qwen thinking output
        output_ids = outputs[0][len(inputs.input_ids[0]):].tolist()
        
        if "qwen" in config['model'].lower() and config.get('enable_thinking', False):
            try:
                # Find </think> token (151668)
                index = len(output_ids) - output_ids[::-1].index(151668)
                output_ids = output_ids[index:]
            except ValueError:
                pass
        
        generation = tokenizer.decode(output_ids, skip_special_tokens=True).strip()
    else:
        # Classic format
        input_template = config['demo_prompt'].replace("{INST}", config['instruction'])\
                                             .replace("{Q}", "{current}")\
                                             .replace("{A}</s>", "")\
                                             .replace("{A}", "")\
                                             .replace("{D}", "{context}").rstrip()
        prompt = input_template.replace("{current}", input_current_text)\
                              .replace("{context}", input_context_text)
        
        inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
        
        with torch.cuda.amp.autocast():
            outputs = model.generate(
                **inputs,
                do_sample=True,
                temperature=config['temperature'],
                top_p=config['top_p'],
                max_new_tokens=config['max_new_tokens'],
                num_return_sequences=1,
                use_cache=True,
            )
        
        generation = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)
    
    return clean_thinking_output(generation.strip())

def analyze_with_mirage(data_item: dict, output_text: str, config: dict) -> dict:
    """Analyze model internals with MIRAGE."""
    # Clear GPU memory before loading
    clear_gpu_memory()
    
    try:
        # Load model with aggressive memory saving
        model_mirage = inseq.load_model(
            config['model'],
            "saliency",
            model_kwargs={
                "device_map": 'auto',
                "max_memory": get_max_memory(),
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "offload_folder": "./offload",
            },
            tokenizer_kwargs={"use_fast": False, "trust_remote_code": True},
        )

        # Load tokenizer separately for stop tokens
        tokenizer_mirage = AutoTokenizer.from_pretrained(config['model'], use_fast=False, trust_remote_code=True)
        
        stop: list[str] = []
        stop_token_ids: list[int] = []
        
        if hasattr(tokenizer_mirage, '_convert_token_to_id'):
            stop_token_ids = [tokenizer_mirage._convert_token_to_id(stop_token) for stop_token in stop if stop_token]
        
        if hasattr(model_mirage.model.config, 'eos_token_id'):
            stop_token_ids.append(model_mirage.model.config.eos_token_id)
        
        stop_token_ids = list(set(stop_token_ids))
        
        if hasattr(tokenizer_mirage, 'unk_token_id') and tokenizer_mirage.unk_token_id in stop_token_ids:
            stop_token_ids.remove(tokenizer_mirage.unk_token_id)
        
        doc_list = data_item['docs']
        input_context_text = "".join([make_doc_prompt(doc, doc_id, config['doc_prompt']) 
                                     for doc_id, doc in enumerate(doc_list)])
        input_current_text = data_item['question']
        
        if config.get('use_chat_template', False):
            input_template = "{context}\n\nQuestion: {current}\n\nAnswer:"
            contextless_input_current_text = "Question: {current}\n\nAnswer:"
        else:
            input_template = config['demo_prompt'].replace("{INST}", config['instruction'])\
                                                 .replace("{Q}", "{current}")\
                                                 .replace("{A}</s>", "")\
                                                 .replace("{A}", "")\
                                                 .replace("{D}", "{context}").rstrip()
            contextless_input_current_text = input_template.replace("{context}", "")
        
        lm_rag_prompting_example = AttributeContextArgs(
            model_name_or_path=config['model'],
            input_context_text=input_context_text,
            input_current_text=input_current_text,
            output_template="{current}",
            input_template=input_template,
            contextless_input_current_text=contextless_input_current_text,
            show_intermediate_outputs=False,
            attributed_fn="contrast_prob_diff",
            context_sensitivity_std_threshold=0,
            output_current_text=output_text,
            attribution_method="saliency",
            attribution_kwargs={"logprob": True},
            save_path=None,
            tokenizer_kwargs={"use_fast": False, "trust_remote_code": True},
            model_kwargs={
                "device_map": 'auto',
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "max_memory": get_max_memory(),
                "load_in_8bit": False,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "offload_folder": "./offload",
            },
            generation_kwargs={
                "do_sample": True,
                "temperature": config['temperature'],
                "top_p": config['top_p'],
                "max_new_tokens": config['max_new_tokens'],
                "num_return_sequences": 1,
                "eos_token_id": stop_token_ids if stop_token_ids else None,
                "use_cache": True,
            },
            decoder_input_output_separator=config.get('decoder_input_output_separator', '\n'),
            special_tokens_to_keep=config.get('special_tokens_to_keep', []),
            show_viz=False,
        )
        
        # Run attribution with memory management
        with torch.cuda.amp.autocast():
            result = attribute_context_with_model(lm_rag_prompting_example, model_mirage)
        
        # Extract attribution data properly
        tokens = result.input_context_tokens if hasattr(result, 'input_context_tokens') else []
        
        # Transform CCIOutput objects into plain dicts
        raw_cci = result.cci_scores if hasattr(result, 'cci_scores') else []
        cci_list = []
        for item in raw_cci:
            cci_list.append({
                "cti_idx": getattr(item, "cti_idx", None),
                "cti_score": getattr(item, "cti_score", None),
                "input_context_scores": list(getattr(item, "input_context_scores", [])),
            })
        
        # CTI scores as floats
        raw_cti = result.cti_scores if hasattr(result, 'cti_scores') else []
        cti_list = []
        for s in raw_cti:
            cti_list.append(getattr(s, "cti_score", s))
        
        attribution_data = {
            "input_context_tokens": list(tokens),
            "cci_scores": cci_list,
            "cti_scores": cti_list
        }
        
    except Exception as e:
        logger.error(f"Attribution error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Return empty attribution data instead of crashing
        attribution_data = {
            "input_context_tokens": [],
            "cci_scores": [],
            "cti_scores": []
        }
    finally:
        # Always cleanup
        if 'model_mirage' in locals():
            del model_mirage
        if 'tokenizer_mirage' in locals():
            del tokenizer_mirage
        clear_gpu_memory()
        
        # Extra delay after MIRAGE processing
        import time
        time.sleep(2)
    
    return attribution_data

def add_citations_to_output_with_debug(output_text: str, attribution_data: dict, data_item: dict, config: dict):
    """Version améliorée avec logging détaillé des scores."""
    if not attribution_data.get("cti_scores") or not attribution_data.get("input_context_tokens"):
        logger.warning("No attribution data available, returning original output")
        return output_text, {}, {}
    
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(config['model'], use_fast=False, trust_remote_code=True)
    
    try:
        # Calculer le seuil CTI
        cti_scores = attribution_data.get("cti_scores", [])
        if isinstance(cti_scores[0], dict):
            cti_scores = [s['cti_score'] for s in cti_scores]
        cti_threshold = np.mean(cti_scores) + config['CTI'] * np.std(cti_scores)
        
        logger.info(f"CTI threshold: {cti_threshold:.4f} (mean: {np.mean(cti_scores):.4f}, std: {np.std(cti_scores):.4f})")
        
        # Détecter les séparateurs de documents
        newline_token = config.get('tokenizer_newline_token', '<0x0A>')
        input_tokens = attribution_data["input_context_tokens"]
        doc_seps = np.array(input_tokens) == newline_token
        
        # Log séparateurs trouvés
        logger.info(f"Found {np.sum(doc_seps)} document separators for {len(data_item['docs'])} documents")
        
        # Si pas assez de séparateurs, essayer d'autres méthodes
        if np.sum(doc_seps) < len(data_item['docs']) - 1:
            logger.warning("Not enough separators found, trying alternative methods")
            # Estimation basée sur la longueur
            tokens_per_doc = len(input_tokens) // len(data_item['docs'])
            doc_seps = np.zeros(len(input_tokens), dtype=bool)
            for i in range(1, len(data_item['docs'])):
                pos = i * tokens_per_doc
                if pos < len(doc_seps):
                    doc_seps[pos] = True
        
        # Traiter chaque phrase
        sents = sent_tokenize(output_text)
        new_output = ""
        start_pos_sent = 0
        doc_scores_per_sentence = {}
        all_doc_scores = {}  # Nouveau: stocker tous les scores pour debug
        
        for sent_idx, sent in enumerate(sents):
            end_pos_sent = start_pos_sent + len(tokenizer.tokenize(sent))
            
            # Obtenir les scores pour cette phrase
            cite_result = mirage_cite(
                attribution_data, 
                cti_threshold, 
                start_pos_sent, 
                end_pos_sent, 
                config['CCI'], 
                doc_seps
            )
            
            # Logger les scores détaillés
            logger.info(f"Sentence {sent_idx}: '{sent[:50]}...'")
            logger.info(f"  Token range: {start_pos_sent}-{end_pos_sent}")
            logger.info(f"  Document scores: {[f'Doc{i}: {score:.4f}' for i, score in enumerate(cite_result)]}")
            
            start_pos_sent = end_pos_sent
            
            # Stocker tous les scores (pas seulement ceux > 0)
            all_doc_scores[sent_idx] = cite_result
            
            if cite_result and any(v > 0 for v in cite_result):
                sent = remove_citations(sent)
                doc_scores_per_sentence[sent_idx] = cite_result
                
                # Obtenir les indices de documents avec scores > 0
                best_doc_id_tmp = {i: v for i, v in enumerate(cite_result) if v > 0}
                if best_doc_id_tmp:
                    best_doc_id = list(dict(sorted(best_doc_id_tmp.items(), 
                                                 key=lambda item: item[1], 
                                                 reverse=True)).keys())
                    
                    # Logger pourquoi certains documents sont choisis
                    logger.info(f"  Selected docs for citation: {best_doc_id} (top {config.get('at_most_citations', 3)})")
                    
                    best_doc_id = best_doc_id[:min(config.get('at_most_citations', 3), len(best_doc_id))]
                    
                    # Créer la chaîne de citations
                    best_doc_id_str = "".join([f"[{i+1}]" for i in best_doc_id])
                    sent = best_doc_id_str + " " + sent
            else:
                logger.info("  No citations added (all scores <= 0)")
            
            new_output += sent + " "
        
        return new_output.rstrip(), doc_scores_per_sentence, all_doc_scores
        
    finally:
        del tokenizer
        clear_gpu_memory()

def add_citations_to_output(output_text: str, attribution_data: dict, data_item: dict, config: dict):
    """Add citations to output based on attribution scores."""
    # If no attribution data, return original text
    if not attribution_data.get("cti_scores") or not attribution_data.get("input_context_tokens"):
        logger.warning("No attribution data available, returning original output")
        return output_text, {}
    
    # Get tokenizer for the model
    tokenizer = AutoTokenizer.from_pretrained(config['model'], use_fast=False, trust_remote_code=True)
    tokenizer.padding_side = "left"
    
    try:
        # Calculate CTI threshold
        cti_scores = attribution_data.get("cti_scores", [])
        if not cti_scores:
            return output_text, {}
            
        if isinstance(cti_scores[0], dict):
            cti_scores = [s['cti_score'] for s in cti_scores]
        cti_threshold = np.mean(cti_scores) + config['CTI'] * np.std(cti_scores)
        
        # Get sentences
        sents = sent_tokenize(output_text)
        
        # Detect document separators
        newline_token = config.get('tokenizer_newline_token', '<0x0A>')
        input_tokens = attribution_data["input_context_tokens"]
        
        # Try to find newline tokens
        doc_seps = np.array(input_tokens) == newline_token
        
        # If no newline tokens found, try alternative approaches
        if not np.any(doc_seps):
            logger.warning(f"Newline token '{newline_token}' not found in tokens")
            
            # Try to find document boundaries by looking for the end of each document
            # This is a heuristic approach based on document structure
            num_docs = len(data_item['docs'])
            
            # Method 1: Look for empty or special tokens that might separate documents
            potential_separators = ['', '\n', '\\n', '<|endoftext|>', '</s>', '<0x0A>', 'Ċ']
            for sep in potential_separators:
                if sep in input_tokens:
                    doc_seps = np.array(input_tokens) == sep
                    if np.sum(doc_seps) >= num_docs - 1:  # Found enough separators
                        logger.info(f"Using '{sep}' as document separator")
                        break
            
            # Method 2: If still no separators found, estimate based on document lengths
            if not np.any(doc_seps):
                logger.warning("No document separators found, estimating boundaries")
                doc_seps = np.zeros(len(input_tokens), dtype=bool)
                
                # Estimate token count per document based on character count
                total_chars = sum(len(doc['title']) + len(doc['text']) for doc in data_item['docs'])
                
                if total_chars > 0:
                    cumulative_pos = 0
                    for _i, doc in enumerate(data_item['docs'][:-1]):  # All but last doc
                        doc_chars = len(doc['title']) + len(doc['text'])
                        doc_ratio = doc_chars / total_chars
                        estimated_pos = int(cumulative_pos + doc_ratio * len(input_tokens))
                        
                        if estimated_pos < len(doc_seps):
                            doc_seps[estimated_pos] = True
                        cumulative_pos = estimated_pos
        
        # Process each sentence
        new_output = ""
        start_pos_sent = 0
        doc_scores_per_sentence = {}
        
        for sent_idx, sent in enumerate(sents):
            end_pos_sent = start_pos_sent + len(tokenizer.tokenize(sent))
            
            # Get citations for this sentence
            cite_result = mirage_cite(
                attribution_data, 
                cti_threshold, 
                start_pos_sent, 
                end_pos_sent, 
                config['CCI'], 
                doc_seps
            )
            
            start_pos_sent = end_pos_sent
            
            if cite_result and any(v > 0 for v in cite_result):
                sent = remove_citations(sent)
                
                # Store scores for this sentence
                doc_scores_per_sentence[sent_idx] = cite_result
                
                # Get document indices with scores > 0
                best_doc_id_tmp = {i: v for i, v in enumerate(cite_result) if v > 0}
                if best_doc_id_tmp:
                    best_doc_id = list(dict(sorted(best_doc_id_tmp.items(), 
                                                 key=lambda item: item[1], 
                                                 reverse=True)).keys())
                    best_doc_id = best_doc_id[:min(config.get('at_most_citations', 3), len(best_doc_id))]
                    
                    # Create citation string
                    best_doc_id_str = "".join([f"[{i+1}]" for i in best_doc_id])
                    sent = best_doc_id_str + " " + sent
            
            new_output += sent + " "
        
        return new_output.rstrip(), doc_scores_per_sentence
        
    finally:
        # Cleanup tokenizer
        del tokenizer
        clear_gpu_memory()

def compute_token_highlights(output_text: str, attribution_data: dict, num_docs: int, tokenizer, config: dict) -> list[dict[str, Any]]:
    """Compute token-level highlights for the attributed answer visualization."""
    if not attribution_data.get("cci_scores"):
        return []
    
    # Get CTI threshold
    cti_scores = attribution_data.get("cti_scores", [])
    if isinstance(cti_scores[0], dict):
        cti_scores = [s['cti_score'] for s in cti_scores]
    cti_threshold = np.mean(cti_scores) + config['CTI'] * np.std(cti_scores)
    
    # Tokenize output
    output_tokens = tokenizer.tokenize(output_text)
    token_highlights = []
    
    # Detect document boundaries in input context
    newline_token = config.get('tokenizer_newline_token', '<0x0A>')
    input_tokens = attribution_data.get("input_context_tokens", [])
    
    # Find document boundaries
    doc_boundaries = []
    current_start = 0
    for i, token in enumerate(input_tokens):
        if token == newline_token:
            doc_boundaries.append((current_start, i))
            current_start = i + 1
    # Add last document
    if current_start < len(input_tokens):
        doc_boundaries.append((current_start, len(input_tokens) - 1))
    
    # Ensure we have the right number of boundaries
    if len(doc_boundaries) != num_docs:
        # Fallback: divide tokens equally
        tokens_per_doc = len(input_tokens) // num_docs
        doc_boundaries = [(i * tokens_per_doc, min((i + 1) * tokens_per_doc, len(input_tokens))) 
                         for i in range(num_docs)]
    
    # Process each output token
    for token_idx, token in enumerate(output_tokens):
        # Find relevant CCI scores for this output token
        doc_scores = np.zeros(num_docs)
        found_attribution = False
        
        for cci_item in attribution_data['cci_scores']:
            if cci_item['cti_idx'] == token_idx and cci_item['cti_score'] >= cti_threshold:
                found_attribution = True
                # Apply CCI filtering
                CCI_value = np.array(cci_item['input_context_scores'])
                
                if config['CCI'] < 0:
                    # Percentage-based threshold
                    cci_threshold = (1 + config['CCI']/100) * np.max(CCI_value) - config['CCI']/100 * np.min(CCI_value)
                elif config['CCI'] == 0:
                    # Mean-based threshold
                    cci_threshold = np.mean(CCI_value)
                else:
                    # Top-k threshold
                    sorted_values = np.sort(CCI_value)
                    if config['CCI'] < len(sorted_values):
                        cci_threshold = sorted_values[-config['CCI']]
                    else:
                        cci_threshold = sorted_values[0]
                
                # Aggregate scores by document
                for doc_idx, (start, end) in enumerate(doc_boundaries):
                    if doc_idx < num_docs:
                        for i in range(start, min(end + 1, len(CCI_value))):
                            if CCI_value[i] >= cci_threshold:
                                doc_scores[doc_idx] += CCI_value[i]
        
        # Find the document with highest score
        if found_attribution and np.max(doc_scores) > 0:
            best_doc = int(np.argmax(doc_scores))
            score = float(doc_scores[best_doc])
            
            # Clean token for display (remove special prefixes)
            clean_token = token
            if clean_token.startswith('Ġ'):
                clean_token = ' ' + clean_token[1:]  # Replace Ġ with space
            elif clean_token.startswith('▁'):
                clean_token = ' ' + clean_token[1:]  # Replace ▁ with space
            elif clean_token.startswith('##'):
                clean_token = clean_token[2:]  # Remove ## for subword tokens
            
            token_highlights.append({
                "token": clean_token,
                "doc_idx": best_doc,
                "score": score,
                "all_doc_scores": doc_scores.tolist()  # Keep all scores for debugging
            })
        else:
            # No attribution for this token
            clean_token = token
            if clean_token.startswith('Ġ') or clean_token.startswith('▁'):
                clean_token = ' ' + clean_token[1:]
            elif clean_token.startswith('##'):
                clean_token = clean_token[2:]
                
            token_highlights.append({
                "token": clean_token,
                "doc_idx": -1,
                "score": 0.0,
                "all_doc_scores": [0.0] * num_docs
            })
    
    return token_highlights

def extract_citations_and_highlights(output_text: str, attribution_data: dict, 
                                           documents: list[Document], doc_scores_per_sentence: dict, 
                                           all_doc_scores: dict, config: dict) -> dict[str, Any]:
    """Version unifiée qui montre les highlights pour TOUS les documents avec des contributions."""
    # Extraire les citations
    citations = []
    sentences = sent_tokenize(output_text)
    
    for sent_idx, sentence in enumerate(sentences):
        citation_pattern = r'\[(\d+)\]'
        cited_docs = re.findall(citation_pattern, sentence)
        if cited_docs:
            doc_indices = [int(d) - 1 for d in cited_docs]
            
            # Obtenir les scores
            scores = []
            if sent_idx in doc_scores_per_sentence:
                sentence_scores = doc_scores_per_sentence[sent_idx]
                for doc_idx in doc_indices:
                    if doc_idx < len(sentence_scores):
                        scores.append(float(sentence_scores[doc_idx]))
                    else:
                        scores.append(0.0)
            else:
                scores = [0.0] * len(doc_indices)
            
            citations.append({
                "sentence_idx": sent_idx,
                "sentence": sentence,
                "cited_docs": doc_indices,
                "scores": scores
            })
    
    # Calculer les highlights pour TOUS les documents
    document_highlights = []
    
    # Obtenir le tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(config['model'], use_fast=False, trust_remote_code=True)
    
    try:
        # Détecter les frontières de documents
        newline_token = config.get('tokenizer_newline_token', '<0x0A>')
        input_tokens = attribution_data.get("input_context_tokens", [])
        
        # Trouver les frontières
        doc_boundaries = []
        current_pos = 0
        
        for i, token in enumerate(input_tokens):
            if token == newline_token:
                doc_boundaries.append((current_pos, i))
                current_pos = i + 1
        
        # Ajouter le dernier document
        if current_pos < len(input_tokens):
            doc_boundaries.append((current_pos, len(input_tokens) - 1))
        
        # Si pas assez de frontières, diviser équitablement
        if len(doc_boundaries) != len(documents):
            logger.warning(f"Boundary mismatch: found {len(doc_boundaries)} boundaries for {len(documents)} documents")
            tokens_per_doc = len(input_tokens) // len(documents)
            doc_boundaries = [(i * tokens_per_doc, min((i + 1) * tokens_per_doc - 1, len(input_tokens) - 1)) 
                             for i in range(len(documents))]
        
        # Calculer le seuil CTI pour filtrer les contributions significatives
        cti_scores = attribution_data.get("cti_scores", [])
        if isinstance(cti_scores[0], dict):
            cti_scores = [s['cti_score'] for s in cti_scores]
        cti_threshold = np.mean(cti_scores) + config['CTI'] * np.std(cti_scores)
        
        # Pour chaque document
        for doc_idx, doc in enumerate(documents):
            highlights = []
            
            # Vérifier si ce document a des contributions (pas seulement s'il est cité)
            has_contributions = False
            
            # Vérifier dans all_doc_scores si le document a des scores > 0
            for sent_idx, scores in all_doc_scores.items():
                if doc_idx < len(scores) and scores[doc_idx] > 0:
                    has_contributions = True
                    logger.info(f"Document {doc_idx} has contribution in sentence {sent_idx}: {scores[doc_idx]:.4f}")
                    break
            
            # Si le document a des contributions, calculer les highlights
            if has_contributions and doc_idx < len(doc_boundaries):
                start_token, end_token = doc_boundaries[doc_idx]
                logger.info(f"Processing highlights for document {doc_idx} (tokens {start_token}-{end_token})")
                
                # Tokenizer le document
                doc_text = doc.text
                doc_tokens = tokenizer.tokenize(doc_text)
                
                # Calculer les scores de contribution pour chaque token du document
                token_contributions = np.zeros(len(doc_tokens))
                
                # Parcourir tous les scores CCI
                for cci_item in attribution_data.get('cci_scores', []):
                    cti_score = cci_item.get('cti_score', 0)
                    
                    if cti_score >= cti_threshold:
                        input_scores = cci_item.get('input_context_scores', [])
                        
                        # Appliquer le seuil CCI
                        if config['CCI'] < 0:
                            cci_threshold = (1 + config['CCI']/100) * np.max(input_scores) - config['CCI']/100 * np.min(input_scores)
                        elif config['CCI'] == 0:
                            cci_threshold = np.mean(input_scores)
                        else:
                            sorted_values = np.sort(input_scores)
                            cci_threshold = sorted_values[-config['CCI']] if config['CCI'] < len(sorted_values) else sorted_values[0]
                        
                        # Accumuler les scores pour ce document
                        for i in range(start_token, min(end_token + 1, len(input_scores))):
                            if input_scores[i] >= cci_threshold:
                                relative_pos = i - start_token
                                if relative_pos < len(token_contributions):
                                    token_contributions[relative_pos] += input_scores[i]
                
                # Convertir en highlights textuels
                if np.max(token_contributions) > 0:
                    highlight_threshold = np.max(token_contributions) * 0.1
                    current_highlight_tokens = []
                    
                    for i, (token, contribution) in enumerate(zip(doc_tokens, token_contributions, strict=False)):
                        if contribution > highlight_threshold:
                            current_highlight_tokens.append((i, token, contribution))
                        else:
                            if current_highlight_tokens:
                                # Créer un highlight
                                start_idx = current_highlight_tokens[0][0]
                                end_idx = current_highlight_tokens[-1][0]
                                
                                highlighted_text = tokenizer.convert_tokens_to_string(doc_tokens[start_idx:end_idx+1])
                                text_start = doc_text.find(highlighted_text)
                                
                                if text_start != -1:
                                    highlights.append({
                                        "start": text_start,
                                        "end": text_start + len(highlighted_text),
                                        "score": float(np.mean([t[2] for t in current_highlight_tokens])),
                                        "text": highlighted_text
                                    })
                                
                                current_highlight_tokens = []
                    
                    # Gérer le dernier groupe
                    if current_highlight_tokens:
                        start_idx = current_highlight_tokens[0][0]
                        end_idx = current_highlight_tokens[-1][0]
                        highlighted_text = tokenizer.convert_tokens_to_string(doc_tokens[start_idx:end_idx+1])
                        text_start = doc_text.find(highlighted_text)
                        
                        if text_start != -1:
                            highlights.append({
                                "start": text_start,
                                "end": text_start + len(highlighted_text),
                                "score": float(np.mean([t[2] for t in current_highlight_tokens])),
                                "text": highlighted_text
                            })
                
                logger.info(f"Document {doc_idx}: Found {len(highlights)} highlights")
            else:
                logger.info(f"Document {doc_idx}: No contributions found")
            
            # Ajouter le document même s'il n'a pas de highlights
            document_highlights.append({
                "doc_idx": doc_idx,
                "title": doc.title,
                "highlights": highlights,
                "full_text": doc.text,
                "has_contributions": has_contributions,
                "is_cited": any(doc_idx in cite['cited_docs'] for cite in citations)
            })
    
    finally:
        del tokenizer
    
    return {
        "citations": citations,
        "document_highlights": document_highlights
    }

# Global variable to store model config for token highlights
config: dict[str, Any] = {}

@app.post("/process", response_model=ProcessResponse)
async def process_endpoint(request: ProcessRequest):
    """Process documents with MIRAGE and return attributed output.

    Args:
        request (ProcessRequest): The incoming processing parameters.

    Returns:
        ProcessResponse: The structured response with output, citations, and highlights.

    Raises:
        HTTPException: If processing fails.
    """
    try:
        result = await process_with_mirage(request)
        return ProcessResponse(**result)
    except Exception as err:
        logger.error(f"Processing error: {err}")
        # Preserve original exception context
        raise HTTPException(status_code=500, detail=str(err)) from err

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Serve the main HTML page."""
    return templates.TemplateResponse("template.html", {"request": request})

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)